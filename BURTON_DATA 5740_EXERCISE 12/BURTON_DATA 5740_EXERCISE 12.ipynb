{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "vdgwu7v7zsaedfl6ce5n",
   "authorId": "8092773048432",
   "authorName": "NWBURTON",
   "authorEmail": "burton.n.w@wustl.edu",
   "sessionId": "fbf0c180-6660-41c2-a291-e77436c248e0",
   "lastEditTime": 1763515474639
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e590ec3a-9eaa-4f6c-aaa8-67213cf22da8",
   "metadata": {
    "name": "intro",
    "collapsed": false
   },
   "source": "# Exercise 12: Clustering to find Anomalies\nYou are working as a data analyst for the Office of the Inspector General in Washington, D.C. Your task is to identify **unusual government purchase card (P-Card) spending patterns** that could indicate potential waste or misuse. Each record in your dataset represents one government **cardholder**. The dataset summarizes their monthly spending behavior."
  },
  {
   "cell_type": "markdown",
   "id": "980c8551-0839-47b2-a01c-d48f7411a32c",
   "metadata": {
    "name": "number_1",
    "collapsed": false
   },
   "source": "Use **DBSCAN** to identify natural clusters and potential outliers among cardholders.\n1. **Import and Explore**  \n    * Load the pcard_summary.csv dataset using pandas  \n    * Examine the distribution of each numeric variable  \n    * Plot pairwise scatterplots or a 3D scatterplot to get a sense of clustering  "
  },
  {
   "cell_type": "code",
   "id": "35cb7c91-f528-4d5b-a75b-796f10174b70",
   "metadata": {
    "language": "python",
    "name": "import_and_explore"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\np_card = pd.read_csv('pcard_summary.csv')\n\n# print pairplot of numeric coefficients\nnum_vars = ['avg_transaction_amount',\n            'transactions_per_month',\n            'pct_weekend_transactions']\n\n# create pairplot\nsns.pairplot(p_card[num_vars])\nplt.show()\n\n# 3D scatter plot\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111,projection='3d')\nax.scatter(p_card[num_vars[0]],\n           p_card[num_vars[1]],\n           p_card[num_vars[2]],\n           s=30)\n\nax.set_xlabel(num_vars[0])\nax.set_ylabel(num_vars[1])\nax.set_zlabel(num_vars[2])\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e40e699-9abf-4c3c-850d-c498045ff789",
   "metadata": {
    "name": "plots_info",
    "collapsed": false
   },
   "source": "#### Distributions\n* **- avg_transaction_amount** is heavily right-skewed with several very large purchases - possible high-risk cases.  \n* **- transactions_per_month** shows two clear populations: light users vs. high-volume purchasers.  \n* **- pct_weekend_transactions** clusters strongly near 0 (most goverment purchasing occurs on weekdays), with a thin tail extending toward high weekend usage.  \n------\n#### Pairplots\n* There appear to be natural groupings along both spending amount and transaction count.\n* Some cardholders combine high transaction amounts with high weekend percentages, making them potential outliers.\n* The large bands suggest clusters, ideal for DBSCAN.\n-----\n#### 3D Plot\n* The cardholders form two main dense clusters, along with several isolated points far from the core.\n* High-spending outliers are clearly visible at the far right of the avg_transaction_amount axis.\n* A small group with unusually high weekend usage also stands apart from the main population."
  },
  {
   "cell_type": "markdown",
   "id": "4e8f6d74-1710-4846-81f4-13d6050ea978",
   "metadata": {
    "name": "num2",
    "collapsed": false
   },
   "source": "**Preprocess**  \n*  Extract the three numeric columns\n*  Scale them using StandardScaler from sklearn.preprocessing."
  },
  {
   "cell_type": "code",
   "id": "889ca8a5-cd27-4f9d-b5b9-4b935ebe9b86",
   "metadata": {
    "language": "python",
    "name": "scale_data"
   },
   "outputs": [],
   "source": "from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX = scaler.fit_transform(p_card[['avg_transaction_amount',\n                                        'transactions_per_month',\n                                        'pct_weekend_transactions']])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bfad3595-8f6e-41db-b4a7-03fa776f6542",
   "metadata": {
    "name": "num3",
    "collapsed": false
   },
   "source": "**3. Parameter Exploration**  \n* Use a **k-distance plot** to estimate a good value for eps. *(Hint: try min_samples=4 and plot the distance to the 4th nearest neighbor.)*\n* Try multiple combinations of eps and min_samples until clusters start to emerge."
  },
  {
   "cell_type": "code",
   "id": "ecd8cb54-1205-412d-b36f-e893bcf02067",
   "metadata": {
    "language": "python",
    "name": "param_exploration"
   },
   "outputs": [],
   "source": "from sklearn.neighbors import NearestNeighbors\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nall_distances = pd.DataFrame()\n\nfor n in range(4,11):\n    neighbors = NearestNeighbors(n_neighbors=n)\n    neighbors_fit = neighbors.fit(X)\n    distances, indices = neighbors_fit.kneighbors(X)\n\n    distances = np.sort(distances[:,n-1])\n    all_distances[n] = distances\n\nall_distances.plot()\nplt.title(\"K-distance graph (for choosing eps)\")\nplt.xlabel(\"Points sorted by distance\")\nplt.ylabel(f\"{n}-th nearest neighbor distance\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e468c0f7-103c-41e8-a84a-5b2ebd8d6ac4",
   "metadata": {
    "name": "k_distance_plot_comment",
    "collapsed": false
   },
   "source": "My choice for an eps based on the k-distance plot is 0.30. This is where the elbow occurs, right before the plot becomes steep."
  },
  {
   "cell_type": "markdown",
   "id": "b88a41e4-c0ab-4e79-968e-4c04903231da",
   "metadata": {
    "name": "num_4",
    "collapsed": false
   },
   "source": "#### 4. Run DBSCAN\n* Use sklearn.cluster.DBSCAN to fit the model.\n* Create a new column for cluster_label in your dataframe"
  },
  {
   "cell_type": "code",
   "id": "43d7c716-a368-475d-bc84-3fff79779f9c",
   "metadata": {
    "language": "python",
    "name": "intitial_dbscan",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from sklearn.cluster import DBSCAN\n\nmin_samples = range(2,10)\neps_range = range(1,40)\n\ndbscan_results = []\n\nfor n in min_samples:\n    for eps in eps_range:\n        # starting values that work well for this scaled data\n        db = DBSCAN(eps=eps/10, min_samples=n)\n        db_labels = db.fit_predict(X)\n\n        unique_labels = set(db_labels)\n        n_clusters = len([l for l in unique_labels if l != -1])\n        n_noise = list(db_labels).count(-1)\n\n        counts = pd.DataFrame(db_labels).value_counts()\n\n        dbscan_results.append([n, eps/10, n_clusters, n_noise, min(counts), max(counts), min(counts)/max(counts)])\n\ndbscan = pd.DataFrame(dbscan_results, columns=['min_samples','eps','n_clusters','noise','min_size','max_size','min_max_ratio'])\n\ndbscan",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c50443d7-c78f-4cb6-a9ed-8e965d7f4ed0",
   "metadata": {
    "name": "best_params",
    "collapsed": false
   },
   "source": "**Best parameters**  \n* min_samples = 3\n* eps = 0.4  \n\nI went with min_samples = 3 and eps = 0.4 because this combination produces a meaningful structure in the data. Instead of collapsing into one giant cluster or creating dozens of tiny ones, it creates three distinct spending lcusters along with a reasonable number of noise points. The 3 clusters closely matches the pairplots and this parameter set gives the clearest separation between normal behavior and unsual P-Card activity."
  },
  {
   "cell_type": "markdown",
   "id": "45b10344-9eff-4cdb-8ac2-d1259eb6f478",
   "metadata": {
    "name": "num_5",
    "collapsed": false
   },
   "source": "#### 5. Analyze Results\n* Count how many clusters and noise points were found.\n* Compute the mean values of each variable per cluster.\n* Visualize the clusters with a scatterplot (color by cluster)"
  },
  {
   "cell_type": "code",
   "id": "86a82691-766a-4ff0-ab62-182e9b87f1b3",
   "metadata": {
    "language": "python",
    "name": "final_dbscan"
   },
   "outputs": [],
   "source": "# starting values that work well for this scaled data\n\n# Helper function to fit DBSCAN and plot by cluster\ndef dbscan_plot(X, eps, min_samples):\n\n    db = DBSCAN(eps=eps, min_samples=min_samples)\n    db_labels = db.fit_predict(X)\n\n    unique_labels = set(db_labels)\n    n_clusters = len([l for l in unique_labels if l != -1])\n    n_noise = list(db_labels).count(-1)\n\n    print(f\"Clusters: {n_clusters}\")\n    print(f\"Noise:    {n_noise}\")\n\n    plt.scatter(p_card[\"transactions_per_month\"], p_card[\"pct_weekend_transactions\"],\n                c=db_labels, cmap=\"tab10\", s=30)\n    plt.xlabel(\"transactions_per_month\")\n    plt.ylabel(\"pct_weekend_transactions\")\n    plt.title(\"DBSCAN clustering\")\n    plt.show()\n\n    return db_labels\n\n# Pick som values from our table above that result in 2 clusters, 0 noise, and balanced cluster ratio\nlabels = dbscan_plot(X, 0.4, 3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb4d8535-fc03-451f-82a2-fc02962f961d",
   "metadata": {
    "language": "python",
    "name": "final_db_scan_3D",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from mpl_toolkits.mplot3d import Axes3D\n\ndef dbscan_plot_3d(X, eps, min_samples):\n    db = DBSCAN(eps=eps, min_samples=min_samples)\n    db_labels = db.fit_predict(X)\n\n    # assign cluster labels to the dataframe\n    p_card[\"cluster_label\"] = db_labels\n\n    # compute clusters and noise counts\n    unique_labels = set(db_labels)\n    n_clusters = len([l for l in unique_labels if l != -1])\n    n_noise = list(db_labels).count(-1)\n\n    print(f\"Clusters found: {n_clusters}\")\n    print(f\"Noise points:   {n_noise}\\n\")\n\n    # compute cluster means\n    print(\"Mean values per cluster:\")\n    display(p_card.groupby(\"cluster_label\")[num_vars].mean())\n\n    # create 3D scatter plot\n    fig = plt.figure(figsize=(8,6))\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.scatter(\n        p_card[num_vars[1]],\n        p_card[num_vars[2]],\n        p_card[num_vars[0]],\n        c=db_labels,\n        cmap=\"tab10\",\n        s=30\n    )\n\n    ax.set_xlabel(num_vars[1])\n    ax.set_ylabel(num_vars[2])\n    ax.set_zlabel(num_vars[0])\n\n    plt.title(f\"DBSCAN 3D (eps={eps}, min_samples={min_samples})\")\n    plt.show()\n\n    return db_labels\n\n\n# run the final model\nlabels = dbscan_plot_3d(X, 0.4, 3)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc959a86-a9ac-45b3-b6d8-d390c173fe11",
   "metadata": {
    "name": "num_6",
    "collapsed": false
   },
   "source": "#### 6. Interpretation\n* Which clusters represent \"typical\" spending?\n* Which cardholders might be outliers (cluster=-1)?\n* Discuss plausible explanations for outlier behavior (e.g., legitimate high-volume spending vs potential misuse)\n----------------------------\nOverall, DBSCAN ended up being a really helpful way to break this dataset into meaningful groups and surface anything that looked unusual. Most cardholders clearly fall into one big “typical” spending pattern, while a couple of smaller clusters show alternate but still reasonable usage styles that probably reflect different job roles or purchasing needs. The real value comes from the noise points—the model isolates the handful of cardholders whose spending looks genuinely out of line with everyone else. These cases don’t automatically mean something is wrong, but they’re definitely the ones I’d want to review more closely to understand whether the behavior is justified or if it points to potential misuse."
  }
 ]
}