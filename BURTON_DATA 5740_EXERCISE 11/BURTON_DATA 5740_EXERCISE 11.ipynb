{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "uu2uoqnxi5de5pft24ia",
   "authorId": "8092773048432",
   "authorName": "NWBURTON",
   "authorEmail": "burton.n.w@wustl.edu",
   "sessionId": "2167f0ac-a2cf-4bcc-9003-9ea593f79239",
   "lastEditTime": 1762665427546
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "54cbd038-ef5e-4d7a-b9fe-c6f99bcc802f",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": "import sys\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom itertools import product\nfrom collections import OrderedDict\nimport xgboost as xgb\nsns.set(rc={'figure.figsize':(16,9)})",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f04d5ba4-e404-46cf-9d77-c8dd0b9a77df",
   "metadata": {
    "language": "python",
    "name": "load_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "election_df = pd.read_csv('county_level_election.csv')\nelection_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed19ce2d-e79f-4caa-8b92-d96d96dc9be0",
   "metadata": {
    "name": "section_02",
    "collapsed": false
   },
   "source": "### Section 2: Bagging / Random Forest\nWe are going to be using test and training splits, cross validation, and fitting a random forest to the data. Create an 80/20 Train/Test split. For accuracy use the .score method.  \n1. Set the number of estimators to be 100, the features to be the square root of available features, and iterate through depths (1-20). Use only 5 folds for cross validation to save some compute resources. Plot the max depth on the x axis and the accuracy on the y axis for training and for the mean cross validation.\n2. Based on the plot, how many nodes would you recommend as the max depth?\n3. What is the accuracy (mean cv) at your chosen depth?\n4. The cross validation looks different than the lab, why?"
  },
  {
   "cell_type": "code",
   "id": "906a040e-0ed7-443a-a61c-bd992d37f641",
   "metadata": {
    "language": "python",
    "name": "set_train_test"
   },
   "outputs": [],
   "source": "X = election_df[['population',\n                 'hispanic',\n                 'minority',\n                 'female',\n                 'unemployed',\n                 'income',\n                 'nodegree',\n                 'bachelor',\n                 'inactivity',\n                 'obesity',\n                 'density',\n                 'cancer']]\ny = election_df['votergap']\n\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1a833d6-fae9-465f-9168-ab6955e86e75",
   "metadata": {
    "language": "python",
    "name": "random_forest",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "depths = list(range(1, 21))\ntrain_scores = []\ncvmeans = []\ncvstds = []\ncv_scores = []\n\nfor depth in depths:\n    \n    print(f'Training with depth: {depth}...')\n    randfor = RandomForestRegressor(n_estimators=100,\n                                   max_features='sqrt',\n                                   max_depth=depth,\n                                   random_state=42)\n\n    # Perform training and 5-fold cross validation\n    train_scores.append(randfor.fit(Xtrain, ytrain).score(Xtrain, ytrain))\n    scores = cross_val_score(estimator=randfor, X=Xtrain, y=ytrain, cv=5)\n\n    cvmeans.append(scores.mean())\n    cvstds.append(scores.std())\n\ncvmeans = np.array(cvmeans)\ncvstds = np.array(cvstds)\n\n# Plot the Mean Accuracy from cross validation with 2 std shaded\nplt.plot(depths, cvmeans, '*-', label=\"Mean CV\")\nplt.fill_between(depths, cvmeans - 2*cvstds, cvmeans + 2*cvstds, alpha=0.3)\n\n# Plot accuracy of a model with depth N\n# against the corss validation of the model with depth N\nylim = plt.ylim()\nplt.plot(depths, train_scores, '-+', label=\"Train\")\nplt.legend()\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Depth\")\nplt.xticks(depths);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5bf861b5-b8cc-463b-b150-65b60b9c6fc4",
   "metadata": {
    "name": "random_forest_interpret",
    "collapsed": false
   },
   "source": "I would recommend 8-9 nodes. At 9 nodes, that is where the curve flattens and additional nodes do not improve accuracy significantly.\n\nThe accuracy at 9 nodes is around 73%.\n\nThe random forest plot looks smoother and more stable because it averages the results of many trees, which reduces variance and overfitting compared to a single decision tree. The cross-validation line doesnâ€™t drop off as sharply since ensemble averaging helps the model generalize better. It also uses 5-fold instead of 10-fold cross-validation, which makes the curve less variable across folds."
  },
  {
   "cell_type": "markdown",
   "id": "bdd7cc06-55ce-4204-802d-c19e8033032a",
   "metadata": {
    "name": "section_03",
    "collapsed": false
   },
   "source": "### Section 3: Boosting / XGBoost  \n5. Use the defaults for most parameters. Iterate through depths (1-20). Use only 5 folds for cross validation to save some compute resources. Plot the max depth on the x axis and the accuracy on the y axis for training and for the mean cross validation.\n6. Based on the plot, how many nodes would you recommend as the max depth?\n7. What is the accuracy (mean cv) at your chosen depth?\n8. The cross validation looks different than random forest, why?"
  },
  {
   "cell_type": "code",
   "id": "2b672997-4937-4755-9ea9-eb8928febdd2",
   "metadata": {
    "language": "python",
    "name": "xg_boost_model"
   },
   "outputs": [],
   "source": "depths = list(range(1, 21))\ntrain_scores = []\ncvmeans = []\ncvstds = []\ncv_scores = []\n\nfor depth in depths:\n    \n    print(f'Training with depth: {depth}...')\n    boosting = xgb.XGBRegressor(max_depth=depth,\n                  random_state=42)\n\n    # Perform training and 5-fold cross validation\n    train_scores.append(boosting.fit(Xtrain, ytrain).score(Xtrain, ytrain))\n    scores = cross_val_score(estimator=boosting, X=Xtrain, y=ytrain, cv=5)\n\n    cvmeans.append(scores.mean())\n    cvstds.append(scores.std())\n\ncvmeans = np.array(cvmeans)\ncvstds = np.array(cvstds)\n\n# Plot the Mean Accuracy from cross validation with 2 std shaded\nplt.plot(depths, cvmeans, '*-', label=\"Mean CV\")\nplt.fill_between(depths, cvmeans - 2*cvstds, cvmeans + 2*cvstds, alpha=0.3)\n\n# Plot accuracy of a model with depth N\n# against the corss validation of the model with depth N\nylim = plt.ylim()\nplt.plot(depths, train_scores, '-+', label=\"Train\")\nplt.legend()\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Depth\")\nplt.xticks(depths);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "700ef1ce-45b1-4ba3-8425-d240ff3709e6",
   "metadata": {
    "name": "xgb_interpret",
    "collapsed": false
   },
   "source": "I'd recommend a max depth of around 3 to 5, since that's where the mean cross-validation score levels off and the model stops improving on unseen data. Beyond that point, the training accuracy keeps climbing toward 1.0, which suggest overfitting.\n\nAt that depth, the mean CV accuracy is around 76%.\n\n\nThe cross-validation curve looks different because XGBoost learns in steps, adding one tree at a time to fix the last one's mistakes. That makes it improve fast at first but also start overfitting sooner. Random forests mix a bunch of trees together, so their results come out smoother and more balanced."
  }
 ]
}