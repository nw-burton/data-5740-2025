{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "zac2gop53saixukdapku",
   "authorId": "8092773048432",
   "authorName": "NWBURTON",
   "authorEmail": "burton.n.w@wustl.edu",
   "sessionId": "7a788117-fbec-4a34-8347-c9b4f169cdf9",
   "lastEditTime": 1760371823878
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5e2d02-d268-400e-8100-c43cfcdc9b2f",
   "metadata": {
    "name": "overview",
    "collapsed": false
   },
   "source": "# Data 5740 - 2025 Midterm\n### Commercial/Industrial Construction:\n### Predicting Customer Retention Spend\n\nYou are an analyst at a commercial/industrial construction firm. Leadership wants to **improve customer retention and expansion**, but doesn't know how to prioritize where to focus these improvements or which focus areas will have the biggest impact on future revenue. After a project finishes, some clients sign additional work within 12 months; others don't. You've been asked to **predict expected next-12-months revenue from each customer** based on project attributes and delivery performance, and to **identify which levers most influence retention spend.**\n\nYour deliverable should be in the form of a Snowflake notebook and follow the **Steps in Model Building** we use in class (included in the rubric below) and demonstrate solid data hygiene, reasoning, and statistical rigor.\n\n### Data provided\nFile: midterm_construction_projects.csv (650 rows; one row per completed project)\n\n**Target:**  \n* **next12mo_spend** - dollars of revenue expected from the customer in the 12 months after the indexed project.\n\n**Possible Features:**  \n* Customer: industry, region  \n* Project: project_type, contract_type, project_size_usd, scoppe_complexity, close_time_days  \n* Delivery: n_change_orders, on_time_milestones_pct, safety_incidents, time_overrun_pct, cost_overrun_pct, payment_delay_days, pm_experience_years, discount_pct, is_union_site  \n* Context: prior_relationship_years, competition_count \n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "e9415516-9873-494b-9624-a686fc66e9ce",
   "metadata": {
    "name": "identify_and_clarify_problem",
    "collapsed": true
   },
   "source": "**1. Identify & Clarify the Problem (10 pts)**  \n* In your own words: business objective, decision context, and why prediction + interpretation matter.\n---\nI am tasked with modeling customer retention revenues based on a number of factors. This model needs to accomplish two things:  \n1. I need to predict expected 12-month revenues from each customer based on various features in the data. These include customer data like industry and region, information about each specific project, information about the delivery of the project, and the context of the project/customer.\n2. I need to identify the most important factors to influence spend. This involves looking at the results of the model and interpreting the weights of each coefficient and its significance.  \n\nI will make my decisions for this model based on the overall predictive accuracy of the model and also by the model's ability to describe the relationships between the variables.\n\nPrediction and interpretation are both important for this exercise because the stated objectives lend themselves to a need to not only predict values, but also understand where to invest retention efforts. Understanding the coefficients and significance for all of the model's components will allow us to understand where to better focus retention efforts in order to maximize 12 month revenues."
  },
  {
   "cell_type": "markdown",
   "id": "af583e60-9d0d-4a2f-8095-511e616c9b5f",
   "metadata": {
    "name": "background",
    "collapsed": true
   },
   "source": "**2. Background (10 pts)**  \n* Briefly describe factors that *could* drive retention spend in construction (cite a source if you like)\n---\nProject size will be important in determining 12-month revenues because a larger project means more revenue.\n\nCustomer satisfaction is often influential on customer retention because satisfied customers are easier to retain. Customer appreciation efforts like discounts can also be heavily influential in retaining top customers. I know from my experience in Supply Chain that when we are evaluating bids and selecting suppliers or contractors, payment discounts are always an influential factor. For example, if there are multiple companies with comparable pricing and all of them offering 2 10/Net 30 discounts, a supplier with 2 15/Net 45 would have a leg up on their competitors. This also ties into project type, a CostPlus project gives more transparency in costs to the customers and a Guarunteed Maximum Price (GMP) project allows the customer to know the maximum they will pay regardless of the actual contract cost.\n\n\nProject management is particularly influential in the construction industry. This includes how well the project stays on schedule and budget, given in our data by the variables **cost_overrun_pct** and **time_overrun_pct**. On time milestones is also important here, as clients are more likely to return to a customer that stays on schedule. This can also relate to project manager experience, under the assumption that a more experienced project manager is better (which is not always the case). But also looking at amemendments made to the original agreement or change orders.\n\nI am expecting that some of the most important drivers in customer retention will be the projects efficiency (time and cost overrun) and customer satisfaction. Some other things I would expect to come into play are project complexity, a more complex project will likely have less competition, and prior relationship - it is harder to lose a customer that you have had a for a long time."
  },
  {
   "cell_type": "markdown",
   "id": "738e1392-a560-4401-9e30-0c3624f1fd66",
   "metadata": {
    "name": "select_variables",
    "collapsed": true
   },
   "source": "**3. Select Variables (10 pts)**  \n* Propose a starting set of predictors. Briefly justify any exclusions or transformations.\n----\nOff the bat, I expect the set of predictors to include project data: industry, region, project_type, contract_type. I will dummy encode these categorical variables. project_size_usd will also be relevant because more revenue is earned from larger projects. \n\nThe customer relationship will also likely be relevant to the model, including predictors prior_relationsip_years, discount_pct, reatined_12mo, and customer_satisfaction.  \n\nThe project delivery and project management variables are ones I am also expecting to be important to the model: safety_incidents, cost_overrun_pct, time_overrun_pct, n_change_orders, pm_experience_years, and on_time_milestones_pct.  \n\nThe final model will likely exclude the following fields: competition_count, is_union_site, close_time_days, and payment_delay_days."
  },
  {
   "cell_type": "markdown",
   "id": "6e09c932-b7de-4d8f-b3d3-078deecef5f3",
   "metadata": {
    "name": "acquire_data",
    "collapsed": false
   },
   "source": "**4. Acquire Data (10 pts)**\n* Load the provided CSV\n* Summarize sample size and data dictionary (your own one-liner per field)."
  },
  {
   "cell_type": "code",
   "id": "a6da0cb8-8b35-4724-a93f-5d0b11263a58",
   "metadata": {
    "language": "python",
    "name": "load_data",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import pandas as pd\n\nconstruction_projects = pd.read_csv('midterm_construction_projects.csv')\n\nconstruction_projects.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57eb8470-af22-4347-87bc-4ca39f4ed3d6",
   "metadata": {
    "language": "python",
    "name": "dataframe_info",
    "codeCollapsed": false,
    "collapsed": true
   },
   "outputs": [],
   "source": "print(construction_projects.shape)\nconstruction_projects.dtypes",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0984f04f-c7b0-42c6-a18e-342c9367b4bf",
   "metadata": {
    "language": "python",
    "name": "cat_and_num",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "categorical_cols = construction_projects[['industry',\n                                         'region',\n                                         'project_type',\n                                         'contract_type',\n                                         'is_union_site',\n                                         'retained_12mo']]\n\nnumeric_cols = construction_projects[['project_size_usd',\n                                     'scope_complexity',\n                                     'close_time_days',\n                                     'prior_relationship_years',\n                                     'competition_count',\n                                     'discount_pct',\n                                     'pm_experience_years',\n                                     'safety_incidents',\n                                     'on_time_milestones_pct',\n                                     'customer_satisfaction',\n                                     'cost_overrun_pct',\n                                     'time_overrun_pct',\n                                     'payment_delay_days',\n                                     'n_change_orders',\n                                     'next12mo_spend']]\n\nnumeric_cols.describe()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0cff645b-4e7e-4d02-b74c-540bf0271bd7",
   "metadata": {
    "name": "num_cols_analysis",
    "collapsed": true
   },
   "source": "There is a total of 650 rows in the dataset. There are missing values for the following columns:  \n* discount_pct  \n* on_time_milestones_pct\n* pm_experience_years  \n\nLooking at **on_time_milestones_pct**, it is scaled 0-100. All other percent variables in the data are scaled 0-1. I will transform this by dividing each value by 100 so all the percents are scaled the same. This will aid in interpretability. "
  },
  {
   "cell_type": "code",
   "id": "719ac730-e269-48c3-a5f2-3e88a9ee8394",
   "metadata": {
    "language": "python",
    "name": "data_transformations",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "construction_projects['on_time_milestones_pct'] = construction_projects['on_time_milestones_pct'] / 100\nconstruction_projects['on_time_milestones_pct'].describe()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1c78d99-03b9-43a8-b880-686f6c122b30",
   "metadata": {
    "language": "python",
    "name": "create_data_dict",
    "codeCollapsed": true,
    "collapsed": true
   },
   "outputs": [],
   "source": "# create disctionary using dtypes and blank description column\ndata_dictionary = pd.DataFrame({\n    \"Field Name\": construction_projects.columns,\n    \"Data Type\": construction_projects.dtypes.astype(str),\n    \"Description\":\"\"\n})\n\n# create descriptions for fields\ndescriptions = {\n    \"project_id\":\"unique identifier for each project\",\n    \"industry\":\"industry sector for the project\",\n    \"region\":\"geographic location (region) of the project\",\n    \"project_type\":\"type of construction project\",\n    \"contract_type\":\"type of contract agreement\",\n    \"is_union_site\":\"union site indicator, 0/1 binary\",\n    \"project_size_usd\":\"total volume of project in dollars\",\n    \"scope_complexity\":\"complexity of project on 1-5 scale\",\n    \"close_time_days\":\"days to close deal with customer\",\n    \"prior_relationship_years\":\"duration in years of relationship with customer prior to closing\",\n    \"competition_count\":\"number of other construction companies evaluated for project\",\n    \"discount_pct\":\"rate (percent) of discount awarded on project\",\n    \"pm_experience_years\":\"years of experience for project manager\",\n    \"safety_incidents\":\"number of safety incidents occured on project\",\n    \"on_time_milestones_pct\":\"percent of project milestones met on schedule\",\n    \"customer_satisfaction\":\"score 1-5 of customer satisfaction\",\n    \"cost_overrun_pct\":\"percent of cost overrun compared to proposal\",\n    \"time_overrun_pct\":\"percent of time overrun compared to budget\",\n    \"payment_delay_days\":\"delay in days of payment received from customer\",\n    \"n_change_orders\":\"number of change orders (or contract amendments) issued\",\n    \"next12mo_spend\":\"dollars of revenue expected from the customer in the 12 months after the indexed project\",\n    \"retained_12mo\":\"customer retained for 12 months indicator, 0/1 binary\"\n}\n\n# add the descriptions to the data dictionary\ndata_dictionary[\"Description\"] = data_dictionary[\"Field Name\"].map(descriptions)\n\nprint(f\"construction_projects contains {len(construction_projects)} rows.\")\ndata_dictionary",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3fa0b19e-2827-4c69-8fb3-3a4749ee7811",
   "metadata": {
    "name": "choose_modeling_approach",
    "collapsed": true
   },
   "source": "**5. Choose Modeling Approach (20 pts)**  \n* Primary: **Multiple Linear Regression** (OLS) predicting next12mo_spend.\n* Mention alternatives you considered (e.g., log-transform target; regularization; or classification for retained_12mo) and explain why OLS is appropriate here.\n---\nI chose to use Ordinary Least Squares (OLS) regression to predict next12mo_spend in its original form. The spend data is already close to normal, so there is no immediate need to transform the data. Keeping the numbers in dollars makes the results of the model easier to communicate to leadership - the predictions will be in real, meaningful units.  \n\nA few other options could be considered for this analysis, but they are less appropriate for the question being asked. A log transformation of the predictor could help if the data were skewed, but it is unnecessary here and would require converting the results back to dollars to communicate the results of the model. Classification models would be well-suited if we were simply trying to predict whether a customer spends or not, but the business question is not *if* a customer will spend in the next 12 months, but *how much* a customer will spend.  \n\nTree-based models can handle nonlinear relationships better, but they are harder to interpret and explain the results to stakeholders. Since the other question asked is which features are the most important to the model, interpretability is key in this exercise. Overall, OLS with the untransformed spend data provides the best balance of accuracy, interpretability, and business relevance."
  },
  {
   "cell_type": "markdown",
   "id": "dc817b5c-9a54-4927-aded-e43d048047bb",
   "metadata": {
    "name": "eda",
    "collapsed": false
   },
   "source": "**6. Exploratory Data Analysis & Assumptions (20 pts)**  \n* Descriptives and **plots** for key variables (distributions, pairwise relationships).\n* Address missingness (where, how much, pattern).\n* Consider transformations (e.g. log of project_size_usd or of the target if skewed).\n* Multicollinearity check (e.g., **VIF**). State modeling assumptions."
  },
  {
   "cell_type": "code",
   "id": "ebbcfe69-10f2-444b-97f1-6a3d16afeed6",
   "metadata": {
    "language": "python",
    "name": "target_variable",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "target = numeric_cols['next12mo_spend']\nnumeric_predictors = numeric_cols.drop(columns = ['next12mo_spend'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af50e25f-9767-4fd7-bc65-67dee49fd9ac",
   "metadata": {
    "language": "python",
    "name": "target_variable_plots",
    "codeCollapsed": true,
    "collapsed": true
   },
   "outputs": [],
   "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize = (12,5))\n\n# First subplot: KDE distribution\nsns.kdeplot(data=construction_projects,\n           x=target,\n           fill=True,\n           ax=axes[0])\n\n# Add mean and median lines\nmean_val = target.mean()\nmedian_val = target.median()\naxes[0].axvline(mean_val,\n                color='blue',\n                linestyle='--',\n                linewidth=1,\n                label=f'Mean: {mean_val:.2f}')\naxes[0].axvline(median_val,\n                color='red',\n                linestyle='-',\n                linewidth=1,\n                label=f'Median: {median_val:.2f}')\naxes[0].legend()\naxes[0].set_title('Distribution of Next 12-Month Spend')\n\n# Second subplot: Boxplot\nsns.boxplot(data=construction_projects,\n           x=target,\n           ax=axes[1],\n           color='lightgray')\naxes[1].set_title('Boxplot of Next 12-Month Spend')\n\n# Adjust layout\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8af8db39-aea0-4b8c-9a04-0787fafe5c67",
   "metadata": {
    "name": "target_variable_analysis",
    "collapsed": true
   },
   "source": "The distribution for the target variable looks fairly bell-shaped and unimodal, the mean and median are very close to eachother. The boxplot has a few outliers, but they don't seem to be very far and likely aren't too influential on the model. The whiskers being of similar lengths implies a decent level of normality in the data. I will move forward without transforming the target variable for now."
  },
  {
   "cell_type": "code",
   "id": "de7c48c8-5165-4cef-860b-d5a887f6b35b",
   "metadata": {
    "language": "python",
    "name": "plots_for_numeric_cols",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "for num_col in numeric_predictors:\n    fig,axes = plt.subplots(1, 2, figsize=(25,10))\n\n    # First subplot: KDE distributions\n\n    # Integer values get funky (i.e. safety incidents), so we will handle those differently\n    if pd.api.types.is_integer_dtype(construction_projects[num_col]):\n        sns.histplot(data=construction_projects,\n                    x=num_col,\n                    discrete=True,\n                    ax=axes[0],\n                    color='gray')\n    else:\n        sns.kdeplot(data = construction_projects,\n                    x = num_col,\n                    color='gray',\n                    fill=True,\n                    ax=axes[0])\n\n    # add mean and median lines\n    mean_val = construction_projects[num_col].mean()\n    median_val = construction_projects[num_col].median()\n    axes[0].axvline(mean_val,\n                    color='blue',\n                    linestyle='--',\n                    linewidth=1,\n                    label=f'Mean: {mean_val:.2f}')\n    axes[0].axvline(median_val,\n                    color='red',\n                    linestyle='-',\n                    linewidth=1,\n                    label=f'Median: {median_val:.2f}')\n    axes[0].legend()\n    axes[0].set_title(f'Distribution of {num_col}')\n\n    # Second subplot: Boxplot\n    sns.boxplot(data = construction_projects,\n               x = num_col,\n               ax=axes[1],\n               color='lightgray')\n    axes[1].set_title(f'Boxplot of {num_col}')\n\n    #Adjust layout\n    plt.tight_layout()\n    plt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8206850c-7126-4129-9622-d537548c1640",
   "metadata": {
    "language": "python",
    "name": "safety_inc_value_counts",
    "codeCollapsed": true,
    "collapsed": true
   },
   "outputs": [],
   "source": "construction_projects['safety_incidents'].value_counts()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b516f4a8-edcd-4f25-874e-7c90483fa33c",
   "metadata": {
    "language": "python",
    "name": "pairplot",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "sns.pairplot(numeric_cols)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3e5cd52-64c3-4ffc-9c4e-c0b76bf4d5e9",
   "metadata": {
    "name": "num_cols_summary",
    "collapsed": true
   },
   "source": "Many of the distributions are fairly normal already, so not many transformations are needed. To preserve interpretability in the model, I will try to minimize transformations. However, there are a few variables I want to transform to better work with the model:  \n* project_size_usd: this field is very heavily skewed with a wide-spread of data. I am going to use a log transform on this.  \n* safety_incidents: this field is tricky because it takes on integer values and they range from 1 to 5. The data is heavily concentrated at 0 incidents with 344. This could heavily interfere with the model. I predict that safety incidents will be important to the overall model, so I will transform this into a binary field that simply takes a 0/1 value indicating if incidents occured. This will give me a 344-306 split, which I will be more usable and still meaningful in analysis.\n* prior_relationship_years: this field has some skew, it is not as extreme as project_size_usd. I will keep this field as-is for the initial model, but if the residuals are skewed, I will revisit this variable's effect on the overall model.  \n\ncost and time overrun look to be almost perfectly correlated, so i will likely exclude one of them from the model. When I run VIF, I will pay close attention to these two."
  },
  {
   "cell_type": "code",
   "id": "9332f9db-290b-4f89-affd-8b947a0c9033",
   "metadata": {
    "language": "python",
    "name": "transform_safety_incidents",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Create binary safety_incidents category\nconstruction_projects['safety_incidents_occurred'] = (construction_projects['safety_incidents'] > 0).astype(int)\n\n# add new field to the categorical_cols variables\ncategorical_cols = categorical_cols.copy()\ncategorical_cols.loc[:, 'safety_incidents_occurred'] = construction_projects['safety_incidents_occurred']\n\ncategorical_cols",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25256ed3-1dd9-4a9c-b51f-1a2a18cc395e",
   "metadata": {
    "language": "python",
    "name": "transform_project_size",
    "codeCollapsed": true,
    "collapsed": true
   },
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# log transform project_size_usd\nconstruction_projects['project_size_usd_norm'] = np.log(construction_projects['project_size_usd'])\n\n# create figure for before and after plotting\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n# Original distribution\nsns.histplot(data=construction_projects,\n             x='project_size_usd',\n             ax=axes[0],\n             color='gray',\n             bins=50)\naxes[0].axvline(construction_projects['project_size_usd'].mean(), color='blue', linestyle='--', label='Mean')\naxes[0].axvline(construction_projects['project_size_usd'].median(), color='red', linestyle='-', label='Median')\naxes[0].set_title('Original Distribution of project_size_usd')\naxes[0].legend()\n\n# Transformed distribution\nsns.histplot(data=construction_projects,\n             x='project_size_usd_norm',\n             ax=axes[1],\n             color='steelblue',\n             bins=50)\naxes[1].axvline(construction_projects['project_size_usd_norm'].mean(), color='blue', linestyle='--', label='Mean')\naxes[1].axvline(construction_projects['project_size_usd_norm'].median(), color='red', linestyle='-', label='Median')\naxes[1].set_title('Log-Transformed Distribution of project_size_usd')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c899c980-84a1-4ac2-9d67-9f80a51f991a",
   "metadata": {
    "name": "transform_project_size_analysis",
    "collapsed": true
   },
   "source": "After log transforming project_size_usd, the data appears fairly normal. I will use this in my model."
  },
  {
   "cell_type": "code",
   "id": "f481b20f-1d2b-4d42-9fe2-81627084fa76",
   "metadata": {
    "language": "python",
    "name": "plots_for_cat_columns",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "for cat_col in categorical_cols.columns:\n    fig, axes = plt.subplots(1, 2, figsize=(25, 10))\n\n    # First subplot: Bar plot of category counts\n    sns.countplot(data=construction_projects,\n                  x=cat_col,\n                  hue=cat_col,\n                  ax=axes[0],\n                  legend=False)\n    axes[0].set_title(f'Frequency of {cat_col}')\n    axes[0].tick_params(axis='x', rotation=45)\n\n    # Second subplot: Boxplot of target variable by category\n    sns.boxplot(data=construction_projects,\n                x=cat_col,\n                hue=cat_col,\n                y=target,\n                ax=axes[1])\n\n    axes[1].set_title(f'next12mo_spend by {cat_col}')\n    axes[1].tick_params(axis='x', rotation=45)\n\n    plt.tight_layout()\n    plt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d533e643-4133-4998-ade8-627a54be4b60",
   "metadata": {
    "name": "cat_plots_analysis",
    "collapsed": true
   },
   "source": "Looking at the frequency and boxplots of the categorical variables, there does not seem to be a striking difference between any of the categories with regard to the target variables. I don't need to do any transformation or editing besides dummy encoding industry, region, project_type, and contract_type. I will do this after imputing the missing values.\n\nI will encode each variable using its highest valued category as the reference column:  \n* Industry: Manufacturing\n* Region: Midwest\n* Project Type: NewBuild\n* Contract: FixedBid"
  },
  {
   "cell_type": "code",
   "id": "767a1b0e-08d9-4660-8929-943cb6987917",
   "metadata": {
    "language": "python",
    "name": "missing_counts",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "construction_projects.isna().sum()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32887c37-3abf-48ce-8c38-5d12fa7ea014",
   "metadata": {
    "language": "python",
    "name": "visualize_missings",
    "codeCollapsed": true,
    "collapsed": true
   },
   "outputs": [],
   "source": "sns.heatmap(construction_projects.isna(), cbar=False)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12e19512-d819-4041-b168-32a9da0fe822",
   "metadata": {
    "name": "missing_analysis",
    "collapsed": true
   },
   "source": "There are missing values in discount_pct (14), pm_experience_years(24), and on_time_milestones_pct (11). Looking at the heatmap, there does not appear to be a pattern among the missing values. I will impute the missing values based on the respective spreads of their data.\n\n* discount_pct: there is a slight right-skew, so the median is safer to impute with than the mean since it is more robust to outliers.\n* pm_experience_years: slight right skew, median will be safer than mean\n* on_time_milestones_pct: fairly symmetric, so I will impute using the average."
  },
  {
   "cell_type": "code",
   "id": "c14d7a15-5ea5-4ee0-958e-b6a22d41a84e",
   "metadata": {
    "language": "python",
    "name": "impute_missings",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "construction_projects['discount_pct_cleaned'] = construction_projects['discount_pct'].fillna(construction_projects['discount_pct'].median())\nconstruction_projects['pm_experience_years_cleaned'] = construction_projects['pm_experience_years'].fillna(construction_projects['pm_experience_years'].median())\nconstruction_projects['on_time_milestones_pct_cleaned'] = construction_projects['on_time_milestones_pct'].fillna(construction_projects['on_time_milestones_pct'].mean())\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8265980e-9c36-4ca0-b0bb-df96d062e2fc",
   "metadata": {
    "language": "python",
    "name": "dummies",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# dummy encode industry and drop Manufacturing as the reference\nconstruction_projects = pd.concat([\n    construction_projects,\n    pd.get_dummies(construction_projects['industry'],prefix='industry').drop('industry_Manufacturing', axis=1)\n], axis=1)\n\n# dummy encode region and drop Midwest as the reference\nconstruction_projects = pd.concat([\n    construction_projects,\n    pd.get_dummies(construction_projects['region'],prefix='region').drop('region_Midwest', axis=1)\n], axis=1)\n\n# dummy encode project_type and drop NewBuild as the reference\nconstruction_projects = pd.concat([\n    construction_projects,\n    pd.get_dummies(construction_projects['project_type'],prefix='project_type').drop('project_type_NewBuild', axis=1)\n], axis=1)\n\n# dummy encode contract_type and drop FixedBid as the reference\nconstruction_projects = pd.concat([\n    construction_projects,\n    pd.get_dummies(construction_projects['contract_type'],prefix='contract_type').drop('contract_type_FixedBid', axis=1)\n], axis=1)\n\nconstruction_projects.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "106b60a1-af2d-49b3-b14d-dab9c2af8c11",
   "metadata": {
    "language": "python",
    "name": "cleaned_df",
    "codeCollapsed": true,
    "collapsed": true
   },
   "outputs": [],
   "source": "cleaned_const_proj = construction_projects[['is_union_site',\n                                           'scope_complexity',\n                                           'close_time_days',\n                                           'prior_relationship_years',\n                                           'competition_count',\n                                           'customer_satisfaction',\n                                           'cost_overrun_pct',\n                                           'time_overrun_pct',\n                                           'payment_delay_days',\n                                           'n_change_orders',\n                                           'next12mo_spend',\n                                           'retained_12mo',\n                                           'safety_incidents_occurred',\n                                           'project_size_usd_norm',\n                                           'discount_pct_cleaned',\n                                           'pm_experience_years_cleaned',\n                                           'on_time_milestones_pct_cleaned',\n                                           'industry_Energy',\n                                           'industry_Food&Beverage',\n                                           'industry_Healthcare',\n                                           'industry_Logistics',\n                                           'industry_Pharma',\n                                           'industry_Technology',\n                                           'region_Northeast',\n                                           'region_South',\n                                           'region_West',\n                                           'project_type_Expansion',\n                                           'project_type_Retrofit',\n                                           'project_type_TenantImprovement',\n                                           'contract_type_CostPlus',\n                                           'contract_type_GMP']]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e11bbf0b-5709-44cb-b630-0e257ac76f91",
   "metadata": {
    "language": "python",
    "name": "VIF",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = cleaned_const_proj.drop(columns=['next12mo_spend'])\n\nX = X.apply(lambda col: col.astype(int) if col.dtype == 'bool' else col)\n\nX = X.apply(pd.to_numeric, errors='coerce')\n\nX = sm.add_constant(X)\n\nvif_data = pd.DataFrame()\nvif_data['Variable'] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range (X.shape[1])]\n\nprint(vif_data)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3dafc07b-81a0-4855-8d77-e8d786a5cc03",
   "metadata": {
    "name": "VIF_summary",
    "collapsed": false
   },
   "source": "As suspected, time_overrun_pct and cost_overrun_pct have very high VIFs, almost identical. The other variables with moderately high VIFs are also fields that could be somewhat related to cost or time overrun (a more or less experienced project manager will be more or less likely to overrun the project.) I will drop cost_overrun_pct from the model since it is likely a factor of time overrun, and then rerun the VIF to see what has changed."
  },
  {
   "cell_type": "code",
   "id": "dd9dba84-462e-47b8-b320-53672c7bcb17",
   "metadata": {
    "language": "python",
    "name": "revised_VIF",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = cleaned_const_proj.drop(columns=['next12mo_spend','cost_overrun_pct'])\n\nX = X.apply(lambda col: col.astype(int) if col.dtype == 'bool' else col)\n\nX = X.apply(pd.to_numeric, errors='coerce')\n\nX = sm.add_constant(X)\n\nvif_data = pd.DataFrame()\nvif_data['Variable'] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range (X.shape[1])]\n\nprint(vif_data)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f4936a9-0e9b-4d95-95d0-bbdd42c67030",
   "metadata": {
    "name": "revised_VIF_summary",
    "collapsed": true
   },
   "source": "After dropping cost_overrun, all of the VIFs are now at an acceptable level."
  },
  {
   "cell_type": "markdown",
   "id": "8b4d95b4-d506-461f-86d3-a98066915db0",
   "metadata": {
    "name": "fit_model",
    "collapsed": false
   },
   "source": "**7. Fit the Model (20 pts)**  \n* Fit your baseline OLS. State the formula/design explicitly (how you encoded categoricals, chosen interactions if any, and any transforms).\n* Report coefficient table with **units/interpretation** (turn slopes into practical business statements)."
  },
  {
   "cell_type": "code",
   "id": "829e1cac-04a7-44db-9063-55cb25cd6be1",
   "metadata": {
    "language": "python",
    "name": "baseline_regression"
   },
   "outputs": [],
   "source": "import statsmodels.formula.api as smf\nimport pandas as pd\n\n# explicitly state regression formula\nformula = 'next12mo_spend ~ scope_complexity + \\\n                            prior_relationship_years + \\\n                            customer_satisfaction + \\\n                            time_overrun_pct + \\\n                            n_change_orders + \\\n                            retained_12mo + \\\n                            safety_incidents_occurred + \\\n                            project_size_usd_norm + \\\n                            discount_pct_cleaned + \\\n                            pm_experience_years_cleaned + \\\n                            on_time_milestones_pct_cleaned + \\\n                            industry_Energy + \\\n                            Q(\"industry_Food&Beverage\") + \\\n                            industry_Healthcare + \\\n                            industry_Logistics + \\\n                            industry_Pharma + \\\n                            industry_Technology + \\\n                            region_Northeast + \\\n                            region_South + \\\n                            region_West + \\\n                            project_type_Expansion + \\\n                            project_type_Retrofit + \\\n                            project_type_TenantImprovement + \\\n                            contract_type_CostPlus + \\\n                            contract_type_GMP'\n\n# fit OLS to model\nbaseline_model = smf.ols(formula=formula, data = cleaned_const_proj).fit()\n\nbaseline_model.summary()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0d230405-bb1c-46f8-8b80-f5e258887f48",
   "metadata": {
    "name": "baseline_summary",
    "collapsed": false
   },
   "source": "Based on the results of the model, the significant terms are the intercept, prior_relationship_years, customer_satisfaction, safety_incidents_occured, project_size_usd_norm, and on_time_milestones_pct_cleaned. This model explains 31% of the variation in the data.  \n1. **Intercept:** The interpretation of the intercept is that if all other coefficients are zero, the next12mo_spend for a customer will be $115,300. Since we established reference categories for our dummy variables, this default value takes on the category of those reference variables. In other words, for a New Build project on a Fixed Bid contract in the Manufacturing Industry located in the Midwest, this is the baseline revenue. The other coefficients for the dummy variables indicate how much the target is influenced by other categories.\n2. **Project Size:** This coefficient is $17,470. Since project_size_usd was log transformed, we can interpret this as meaning that for every time the project_size increases by a factor of e (~2.178), the target variable increases by 17,470.\n3. **Prior Relationship:** For every additional year a client has been retained, their next12mo_spend increases by 8,078.\n4. **Customer Satisfaction:** For every additional point of customer satisfaction, next12mo_spend increases by $26,640.\n5. **On Time Milestones:** For every unit increase of on_time milestones, spend increases by 126,300. Since this variable is scaled to 0-1, if 100% of milestones are met on time, that amount is added to the baseline spend.\n6. **Number of Change Orders:** For every additional change order issued, the revenue drops by about $3,550.\n7. **Safety Incidents:** If one or more safety incidents occurs, it negatively impacts next 12-mo spend by 43,280.  \n\n**Industry Dummies**  \nMost of the industry dummies have very high p-values. Since Energy is close to significant, and manufacturing is build into the constant, I don't want to drop the industry dummies, but I will try combining them to see if they have more predictive power. I will combine Energy, Logistics, and Technology into Industrial Tech, Healthcare and Pharma into Health, and Food&Beverage into Consumer.\n\n**Drop some variables with low significance**  \nI will exclude from the final model some of the variables that do not carry very much predictive power in the model. Scope_complexity, time_overrun_pct, retained_12mo. Although industry seems to have a few dummies that are close to significant, region does not, so I will drop the region variables from the model as well. I want to keep project_type and contract_type because logically thinking, these could have some effect on revenues and project scale.  \n\n**Impute using MICE**  \nI will rerun the analysis using MICE imputation on missing values to see if I can improve the performance of those variables.\n\n**Sqrt-Transform Prior Relationship**  \nCalling back to earlier, prior_relationship_years is slightly skewed so I will sqrt transform it. Since it is significant to the model, transforming this data may improve the model's overall effectiveness."
  },
  {
   "cell_type": "markdown",
   "id": "1d344c07-17ce-49a3-922c-8887fb332cd2",
   "metadata": {
    "name": "diagnostics",
    "collapsed": false
   },
   "source": "**8. Diagnostics (20 pts)**  \n* Residuals vs. fitted, Q-Q plot, influence (e.g., Cook's distance), heteroscedasticity check.\n* Comment on where assumptions look OK vs. violated."
  },
  {
   "cell_type": "code",
   "id": "99c69bc2-03cb-46d6-b662-8b9091aaa944",
   "metadata": {
    "language": "python",
    "name": "diagnostic_variables"
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nplt.style.use('seaborn-v0_8')\nplt.rc('figure', titlesize=18)\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=18)\n\n# fitted values (need a constant term for intercept)\nmodel_fitted_y = baseline_model.fittedvalues\n\n# model residuals\nmodel_residuals = baseline_model.resid\n\n# normalized residuals\nmodel_norm_residuals = baseline_model.get_influence().resid_studentized_internal\n\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n\n# leverage, from statsmodels internals\nmodel_leverage = baseline_model.get_influence().hat_matrix_diag\n\n# cook's distance, from statsmodels internals\nmodel_cooks = baseline_model.get_influence().cooks_distance[0]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aac54b0f-b3f0-4705-b6e5-4d6868ce40d4",
   "metadata": {
    "language": "python",
    "name": "residuals_v_fitted",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "plot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(8)\nplot_lm_1.set_figwidth(12)\n\nplot_lm_1.axes[0] = sns.residplot(x =model_fitted_y,\n                                  y ='next12mo_spend',\n                                  data=cleaned_const_proj,\n                                  lowess=True,\n                                  scatter_kws={'alpha': 0.5},\n                                  line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')\n\n\n# annotations\nabs_resid = model_abs_resid.sort_values(ascending=False)\nabs_resid_top_3 = abs_resid[:3]\n\nfor i in abs_resid_top_3.index:\n    plot_lm_1.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_residuals[i]));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e15768dd-1d9d-49f0-ad30-22b6c76560c1",
   "metadata": {
    "name": "res_vs_fit_summary",
    "collapsed": false
   },
   "source": "The Residuals vs Fitted plot looks pretty good overall. Most of the points are scattered randomly around zero, which means the model is doing a decent job capturing the main pattern in the data. There’s a little bit of a curve in the red line, so the model might be missing a small nonlinear trend, but it doesn’t look serious. A few points stand out as possible outliers, but nothing too extreme. The leverage plot will tell us if these points impact the model too much."
  },
  {
   "cell_type": "code",
   "id": "5d2d3a34-4c04-4882-9597-766a872e334b",
   "metadata": {
    "language": "python",
    "name": "qq_plot"
   },
   "outputs": [],
   "source": "QQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nplot_lm_2.set_figheight(8)\nplot_lm_2.set_figwidth(12)\n\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\n\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1eab0372-36d7-4784-80ae-27975ed8443b",
   "metadata": {
    "name": "qq_plot_summary",
    "collapsed": false
   },
   "source": "The residuals from the model are approximately normally distributed, which supports the assumption of normality. However, a few observations deviate from this pattern, suggesting potential outliers that may be influencing the model’s fit. Next steps are to look at cook's distance and leverage for these points."
  },
  {
   "cell_type": "code",
   "id": "17ca6727-aeab-4012-afb6-e86dd54dc876",
   "metadata": {
    "language": "python",
    "name": "scale_location_plot"
   },
   "outputs": [],
   "source": "plot_lm_3 = plt.figure(3)\nplot_lm_3.set_figheight(8)\nplot_lm_3.set_figwidth(12)\n\nplt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\nsns.regplot(x=model_fitted_y, y=model_norm_residuals_abs_sqrt, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_3.axes[0].set_title('Scale-Location')\nplot_lm_3.axes[0].set_xlabel('Fitted values')\nplot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n\n# annotations\nabs_sq_norm_resid = np.flip(np.argsort(model_norm_residuals_abs_sqrt), 0)\nabs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n\nfor i in abs_norm_resid_top_3:\n    plot_lm_3.axes[0].annotate(i, \n                               xy=(model_fitted_y[i], \n                                   model_norm_residuals_abs_sqrt[i]));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf0b9a11-7e69-43d6-975c-05b934379c21",
   "metadata": {
    "name": "scale_location_summary",
    "collapsed": false
   },
   "source": "The Scale-Location plot also looks fine. The points are spread out fairly evenly across the fitted values, and the red line stays mostly flat, which suggests the variance of the residuals is consistent. That means the model isn’t showing big signs of heteroscedasticity. There are a couple of higher points, but overall the spread looks random and balanced."
  },
  {
   "cell_type": "code",
   "id": "1a21e93f-26a5-4afb-a496-39edd451816d",
   "metadata": {
    "language": "python",
    "name": "res_vs_lev_plot"
   },
   "outputs": [],
   "source": "plot_lm_4 = plt.figure(4)\nplot_lm_4.set_figheight(8)\nplot_lm_4.set_figwidth(12)\n\nplt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\nsns.regplot(x=model_leverage, y=model_norm_residuals, \n            scatter=False, \n            ci=False, \n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_4.axes[0].set_xlim(0.02, 0.09)\nplot_lm_4.axes[0].set_ylim(-3, 4)\nplot_lm_4.axes[0].set_title('Residuals vs Leverage')\nplot_lm_4.axes[0].set_xlabel('Leverage')\nplot_lm_4.axes[0].set_ylabel('Standardized Residuals')\n\n# annotations\nleverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n\nfor i in leverage_top_3:\n    plot_lm_4.axes[0].annotate(i, \n                               xy=(model_leverage[i], \n                                   model_norm_residuals[i]))\n    \n# shenanigans for cook's distance contours\ndef graph(formula, x_range, label=None):\n    x = x_range\n    y = formula(x)\n    plt.plot(x, y, label=label, lw=1, ls='--', color='red')\n\np = len(baseline_model.params) # number of model parameters\n\ngraph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50), \n      'Cook\\'s distance') # 0.5 line\n\ngraph(lambda x: np.sqrt((1 * p * (1 - x)) / x), \n      np.linspace(0.001, 0.200, 50)) # 1 line\n\nplt.legend(loc='upper right');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8598d999-abb0-4085-affa-448d4a658700",
   "metadata": {
    "name": "res_vs_lev_summary",
    "collapsed": false
   },
   "source": "The Residuals vs Leverage plot looks fine overall. Most of the points are clustered toward the lower end of leverage, which means most observations don’t have much influence on the model. The red line stays close to zero, showing no strong pattern, which is good. A few points like 139, 273, and 504 stand out a bit, but they don’t appear to be extreme enough to cause major concern. Overall, there’s no clear sign of influential outliers or leverage problems, so the model seems pretty stable.\n\n---\nThe diagnostics do not provide evidence of any strong violations of model assumptions.\n"
  },
  {
   "cell_type": "markdown",
   "id": "a0ab6fd7-0a5f-4207-8377-5174e3519047",
   "metadata": {
    "name": "address_deficiencies",
    "collapsed": false
   },
   "source": "**9. Address Deficiencies (20 pts)**\n* Reasoned iteration: try alternative feature set(s), transformations (e.g., mean/median vs. simple MICE via statsmodels or sklearn imputation).\n* If you try ridge/lasso for stability, show how conclusions change (or don't).\n\n---\n\nOverall, the baseline model after appears to be a pretty decent fit. It highlights a few significant coefficients, explains 31.5% of the variation in the data, and does not violate any model assumptions. I will try a few additional enhancements and imputations to see if I can further improve the model.  "
  },
  {
   "cell_type": "code",
   "id": "85542d26-d410-4432-b772-073fe6cfaae4",
   "metadata": {
    "language": "python",
    "name": "combine_industries"
   },
   "outputs": [],
   "source": "# combine industries into broader categories\nconstruction_projects['industry_IndustrialTech'] = construction_projects[['industry_Energy', 'industry_Logistics', 'industry_Technology']].max(axis=1)\nconstruction_projects['industry_Health'] = construction_projects[['industry_Healthcare', 'industry_Pharma']].max(axis=1)\nconstruction_projects['industry_Consumer'] = construction_projects['industry_Food&Beverage']\n\nconstruction_projects",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44f7b3e1-b474-4274-ab78-616d7d5a7a4a",
   "metadata": {
    "language": "python",
    "name": "sqrt_transform_prior_relationship",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#use square root transformation on prior_relationship_years\nconstruction_projects['prior_relationship_years_norm'] = np.sqrt(construction_projects['prior_relationship_years'])\n\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n# Original distribution\nsns.histplot(data=construction_projects,\n             x='prior_relationship_years',\n             ax=axes[0],\n             color='gray',\n             bins=50)\naxes[0].axvline(construction_projects['prior_relationship_years'].mean(), color='blue', linestyle='--', label='Mean')\naxes[0].axvline(construction_projects['prior_relationship_years'].median(), color='red', linestyle='-', label='Median')\naxes[0].set_title('Original Distribution of prior_relationship_years')\naxes[0].legend()\n\n# Transformed distribution\nsns.histplot(data=construction_projects,\n             x='prior_relationship_years_norm',\n             ax=axes[1],\n             color='steelblue',\n             bins=50)\naxes[1].axvline(construction_projects['prior_relationship_years_norm'].mean(), color='blue', linestyle='--', label='Mean')\naxes[1].axvline(construction_projects['prior_relationship_years_norm'].median(), color='red', linestyle='-', label='Median')\naxes[1].set_title('Log-Transformed Distribution of prior_relationship_years_norm')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3fc11f7b-d65e-440c-a8af-3ea07c8f3624",
   "metadata": {
    "language": "python",
    "name": "final_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# create final dataset\nconstruction_projects_final = construction_projects[['customer_satisfaction',                                                     \n                                                     'n_change_orders',\n                                                     'safety_incidents_occurred',\n                                                     'project_size_usd_norm',\n                                                     'discount_pct',\n                                                     'pm_experience_years',\n                                                     'on_time_milestones_pct',\n                                                     'prior_relationship_years_norm',\n                                                     'industry_IndustrialTech',\n                                                     'industry_Health',\n                                                     'industry_Consumer',\n                                                     'project_type_Expansion',\n                                                     'project_type_Retrofit',\n                                                     'project_type_TenantImprovement',\n                                                     'contract_type_CostPlus',\n                                                     'contract_type_GMP',\n                                                     'next12mo_spend']]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a344a90-5e60-4806-87ee-268512065800",
   "metadata": {
    "language": "python",
    "name": "final_regression",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import statsmodels.api as sm\nfrom statsmodels.imputation import mice\n\nimp = mice.MICEData(construction_projects_final)\nfml = 'next12mo_spend ~ customer_satisfaction + \\\n                            n_change_orders + \\\n                            safety_incidents_occurred + \\\n                            project_size_usd_norm + \\\n                            discount_pct + \\\n                            pm_experience_years + \\\n                            on_time_milestones_pct + \\\n                            prior_relationship_years_norm + \\\n                            industry_IndustrialTech + \\\n                            industry_Consumer + \\\n                            project_type_Expansion + \\\n                            project_type_Retrofit + \\\n                            project_type_TenantImprovement + \\\n                            contract_type_CostPlus + \\\n                            contract_type_GMP'\n\nmice = mice.MICE(fml, sm.OLS, imp)\nfinal_model = mice.fit(10,10)\nfinal_model.summary()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b7edd5c-b8ba-4461-9b0b-a236d1fbfb14",
   "metadata": {
    "language": "python",
    "name": "r2_for_mice",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import patsy\nimport numpy as np\n\n# Build design matrix from the formula and the dataset\ny, X = patsy.dmatrices(fml, construction_projects_final, return_type='dataframe')\n\ny_hat = X @ final_model.params\n\nss_res = ((y.values.flatten() - y_hat.values.flatten())**2).sum()\nss_tot = ((y.values.flatten() - y.values.flatten().mean())**2).sum()\nr_squared = 1 - ss_res/ss_tot\n\nn = X.shape[0]  # number of observations\np = X.shape[1] - 1  # number of predictors (excluding intercept)\nadj_r_squared = 1 - (1 - r_squared)*(n-1)/(n-p-1)\n\nprint(\"R-squared:\", r_squared)\nprint(\"Adjusted R-squared:\", adj_r_squared)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "131fb39d-6c8d-42e0-befb-ce2060cba2c4",
   "metadata": {
    "name": "final_summary",
    "collapsed": true
   },
   "source": "For the final model, I combined the industry columns into broader columns, dropped a few insignificant variables, and transformed the prior_relationship variable to normalize the data. I also used MICE to impute the missing values than I had previously imputed using mean/median.\n\nOverall, this model performed slightly worse than the original model, but not by a lot. The original model had an R2 of 0.315, and the final model had that of 0.306.\n\nThe MICE imputation likely didn't have much effect on the model because the intial data had very few missing values. None of the fields that were missing values were overly skewed and there was no pattern to the missingness. Due to this, the mean and median worked well-enough, and there wasn't signifcant improvement to be made by using MICE.\n\nIn my initial exploratory data analysis, I did some cleaning and data preparation already. Having re-scaled the on_time_milestones_pct column, created a binary safety_incidents column, and transformed the project_size_usd column, the only value left to transform was prior_relationship_years. This predictor was already signficant in the model, and the diagnostic plots didn't show any violations of assumptions, so any change to this variable likely would have had minimal effect on the model anyway.\n\nFor the coefficients dropped, this is fairly risky with OLS because of the variables interactions with each other. The constant lost significance in the final model. Since the coefficient captured the references for the categorical variables, I think it would be better to keep the model with the more significant constant and all of the categorical variables for region, industry, project_type, and contract_type.\n\nOverall, I will go with my baseline model, more explainability of the overall variance and adequate direction for informing the business."
  },
  {
   "cell_type": "markdown",
   "id": "4d0ab5c3-9a04-4cb3-96c4-0f2df5fd619e",
   "metadata": {
    "name": "interpret_and_communicate",
    "collapsed": false
   },
   "source": "**10. Interpret & Communicate (30 pts)**\n* Summarize what drives retention spend and how confident you are.\n* Provide 2-3 actionable recommendations (e.g., reduce overruns, improve on-time milestones, PM staffing).\n* Include a short \"for executives\" paragraph with the single clearest takeaway."
  },
  {
   "cell_type": "markdown",
   "id": "bbccd3f5-2848-4a0a-b075-be0156cbd6c6",
   "metadata": {
    "name": "exercise_summary",
    "collapsed": false
   },
   "source": "### Drivers of Retention Spend  \n*Higher customer satisfaction*     \nThis strongly increases future spend. Each satisfaction point translates to anywhere from +\\$19.1K to \\$34.2K in additional spend.  \n\n*Longer prior relationship*  \nRetention spend is boosted by \\$6.2K-\\$10K for every year a customer has had a relationship with the company.  \n\n*Larger project size*  \nLarger projects generate more spend for the company. Referencing the normalized spend variable, the project increases \\$114-$235 for every 1% increase in project size.    \n\n*Better on-time milestone performance*   \nSignificantly increases spend. If a project’s milestones are met 100% on-time, their next 12-month spend is \\$81.4K-\\$171K higher. \n\n*Safety incidents*  \nThe occurrence of even 1 safety incident translates to a \\$33.4K to \\$53.2K loss in revenue.  \n\n*More Change Orders*  \nEach change order slightly decreases retention spend by \\$266 to \\$6,833.  \n\n### Confidence  \nThe model is stastically strong overall. The combination of the model's F-statistic and low p-value, means that the factors in this model reliably explain the differences in future client spend. The model captures meaningful relationships between some of the variables, but it only explains 31.3% of the overall variance so there are likely other non-measured factors at play. Overall, the confidence in the direction and the significance of the key predictors is high so I am confident in these levers.\n\n### Business Recommendations\n1.\t**Prioritize safety management:** The occurrence of even one safety incident predicts a ~$43K drop in future spend. Reinforce safety training among field workers.\n2.\t**Improve on-time performance:** Meeting milestones on schedule has a major impact on retention. Prioritize adherence to schedules and invest in project management tools or staffing to improve overall delivery.\n3.\t**Strengthen customer relationship:** Longer partnerships clearly pay off. Build account retention programs that focus on maintaining relationship after a project’s completion.\n\n### Executive Summary\nThe single clearest takeaway of this analysis is that customer relationships are the strongest driver of future retention spend. Nearly every major lever ties back to client satisfaction – either directly (through prior relationships and satisfaction scores) or indirectly (through project delivery factors like safety, change orders, and on-time milestones). This suggests that maintaining and strengthening client trust has the greatest long-term revenue impact. Going forward, resources should be focused on understanding and improving the key drivers of customer satisfaction, since it sits at the center of nearly every factor that influence repeat business.\n"
  }
 ]
}