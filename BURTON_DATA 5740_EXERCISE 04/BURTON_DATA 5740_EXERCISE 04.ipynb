{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "zaxwx76kw2ekdrvky5tl",
   "authorId": "8092773048432",
   "authorName": "NWBURTON",
   "authorEmail": "burton.n.w@wustl.edu",
   "sessionId": "c24d4f45-5605-4195-b249-3e456f2f381f",
   "lastEditTime": 1758484253636
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "cf27805c-b0d8-45a2-8c98-c5df61080b89",
   "metadata": {
    "language": "python",
    "name": "data"
   },
   "outputs": [],
   "source": "import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.imputation import mice\n\n# load and clean cars data\ndf1 = pd.read_csv(\"cars.csv\")\n\n#clean data\ndf1[\"Mileage\"] = df1[\"Mileage\"].str.rstrip(\" kmpl\")\ndf1[\"Mileage\"] = df1[\"Mileage\"].str.rstrip(\" km/g\")\ndf1[\"Engine\"] = df1[\"Engine\"].str.rstrip(\" CC\")\ndf1[\"Power\"] = df1[\"Power\"].str.rstrip(\" bhp\")\ndf1[\"Power\"]= df1[\"Power\"].replace(regex=\"null\", value = np.nan)\ndf1[\"Fuel_Type\"]=df1[\"Fuel_Type\"].astype(\"category\")\ndf1[\"Transmission\"]=df1[\"Transmission\"].astype(\"category\")\ndf1[\"Owner_Type\"]=df1[\"Owner_Type\"].astype(\"category\")\ndf1[\"Mileage\"]=df1[\"Mileage\"].astype(\"float\")\ndf1[\"Power\"]=df1[\"Power\"].astype(\"float\")\ndf1[\"Engine\"]=df1[\"Engine\"].astype(\"float\")\ndf1[\"Company\"]=df1[\"Name\"].str.split(\" \").str[0]\ndf1[\"Model\"]=df1[\"Name\"].str.split(\" \").str[1]+df1[\"Name\"].str.split(\" \").str[2]\n\n# Normalize price using log transformation\ndf1['Price_Norm'] = df1['Price'].apply(lambda x: np.log(x))\n\n# Plot the price data before and after log transformation\nfig, axes = plt.subplots(1, 2, figsize = (10,8))\n\nfig.suptitle('Price Before and After Log Transformation')\n\nsns.histplot(df1['Price'], kde = True, ax = axes[0])\naxes[0].set_title('Original Price')\naxes[0].set_xlabel('Price')\naxes[0].set_ylabel('Frequency')\n\nsns.histplot(df1['Price_Norm'], kde = True, ax = axes[1])\naxes[1].set_title('Norm Price')\naxes[1].set_xlabel('Price')\naxes[1].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7759e8ab-3bcc-4850-beca-8c3987b9ab5d",
   "metadata": {
    "language": "python",
    "name": "missing_vals",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def missing_values(df):\n    numrows = len(df)\n    for col in df.columns:\n        num_vals = df1[col].count()\n        num_missing = numrows - num_vals\n        print(f\"{col}: {num_vals} out of {numrows}. {num_missing} missing values.\")\n\nmissing_values(df1)\ndf1.head()\ndf1.describe()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33db0130-3344-415a-9a1c-df36eebbc18e",
   "metadata": {
    "name": "Missing_vals_resp",
    "collapsed": false
   },
   "source": "Power is missing 143 values.\n\nEngine is missing 36 values.\n\nThe column with the most missing values is New_Price, with 5,195. We can drop this column."
  },
  {
   "cell_type": "code",
   "id": "e90ba2e5-1f62-49c3-9533-6a59b46b3a11",
   "metadata": {
    "language": "python",
    "name": "drop_vals"
   },
   "outputs": [],
   "source": "# drop na values\ndf1_cleaned = df1[['Power', 'Engine', 'Mileage','Kilometers_Driven','Year','Price_Norm']].dropna()\nX1 = df1_cleaned[['Power', 'Engine', 'Mileage','Kilometers_Driven','Year']]\ny1 = df1_cleaned['Price_Norm']\n\n# fit OLS model with intercept and 4 Betas\nX1 = sm.add_constant(X1)\nprice_model1 = sm.OLS(y1,X1).fit()\nprice_model1.summary()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5bb9f2d-bef0-4399-9d84-cbf0987733b0",
   "metadata": {
    "name": "drop_vals_resp",
    "collapsed": false
   },
   "source": "After dropping the rows with missing values, the model was trained on 5,874 observations. The model has an R-Squared value of 0.833, meaning 83.3% of the variance is explained by the model. The P-Values for both Mileage and KM Driven are high, indicating that neither of them are statistically significant."
  },
  {
   "cell_type": "code",
   "id": "fd8c545b-432f-483c-8321-4ac7ef57c30d",
   "metadata": {
    "language": "python",
    "name": "nan_means",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# replace missing values with column means\n# df2 = df1.fillna(df1.mean()) <-- got an error because name column is string\n\n# Create new dataframe first\ndf_avgs = df1[['Power','Engine', 'Mileage','Kilometers_Driven','Year','Price_Norm']]\ndf_avgs = df_avgs.fillna(df_avgs.mean())\ndf_avgs.describe()\n\n# create model\nX2 = df_avgs[['Power', 'Engine', 'Mileage', 'Kilometers_Driven', 'Year']]\ny2 = df_avgs[['Price_Norm']]\n# fit OLS model with intercept and 4 Betas\nX2 = sm.add_constant(X2)\nprice_model2 = sm.OLS(y2,X2).fit()\nprice_model2.summary()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "70b9751c-0041-4633-9d1c-4a1fca3a7e95",
   "metadata": {
    "name": "nan_means_resp",
    "collapsed": false
   },
   "source": "6019 rows were used to train the model. The R-Squared is now 0.828, so 82.8% of the variance is now explained. The P-Values for Mileage and KM Driven are still high, so these coefficients are still not significant to the model."
  },
  {
   "cell_type": "code",
   "id": "253ad80a-c5ef-4023-afec-005644291600",
   "metadata": {
    "language": "python",
    "name": "mice_dataframe",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "mice_df = df1[['Power', 'Engine', 'Mileage', 'Kilometers_Driven', 'Year', 'Price_Norm']]\nimp = mice.MICEData(mice_df)\nfml = 'Price_Norm ~ Power + Engine + Mileage + Kilometers_Driven + Year'\nmice = mice.MICE(fml, sm.OLS, imp)\nresults3 = mice.fit(10,10)\nresults3.summary()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72c58d26-5e43-4e9d-acf0-71e41dffa1be",
   "metadata": {
    "name": "mice_resp",
    "collapsed": false
   },
   "source": "From the original model, the parameters have changed slightly. The intercept has moved slightly lower, mileage and KM_Driven have less effect and are still not statistically significant, and the year impacts the price by about 9."
  },
  {
   "cell_type": "code",
   "id": "52980a30-5249-4615-ba67-14eed812cbb6",
   "metadata": {
    "language": "python",
    "name": "mice_dist",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from statsmodels.imputation import mice\n\nmice_df = df1[['Power', 'Mileage', 'Kilometers_Driven', 'Year', 'Engine', 'Price_Norm']]\nimp = mice.MICEData(mice_df)\n\n# Plot the price data before and after log transformation\nfig, axes = plt.subplots(2, 2, figsize = (10,8))\n\nfig.suptitle('Power and Engine With and Without Mice')\n\nsns.histplot(df1['Power'], kde = True, ax = axes[0,0])\naxes[0,0].set_title('Original Power')\naxes[0,0].set_xlabel('Power')\naxes[0,0].set_ylabel('Frequency')\n\nsns.histplot(imp.data['Power'], kde = True, ax = axes[0,1])\naxes[0,1].set_title('Power with Mice')\naxes[0,1].set_xlabel('Power')\naxes[0,1].set_ylabel('Frequency')\n\nsns.histplot(df1['Engine'], kde = True, ax = axes[1,0])\naxes[1,0].set_title('Original Engine')\naxes[1,0].set_xlabel('Engine')\naxes[1,0].set_ylabel('Frequency')\n\nsns.histplot(imp.data['Engine'], kde = True, ax = axes[1,1])\naxes[1,1].set_title('Engine with Mice')\naxes[1,1].set_xlabel('Engine')\naxes[1,1].set_ylabel('Frequency')\n\nplt.tight_layout()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af6b674b-d880-416b-800a-1b85695321ce",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Power overlay\nsns.histplot(df1['Power'], kde=True, color='blue', label='Original Power', bins=20, alpha=0.5)\nsns.histplot(imp.data['Power'], kde=True, color='orange', label='Power with MICE', bins=20, alpha=0.5)\nplt.title('Distribution of Power: Original vs. MICE Imputed')\nplt.xlabel('Power')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n# Engine overlay\nplt.figure(figsize=(12,5))\nsns.histplot(df1['Engine'], kde=True, color='blue', label='Original Engine', bins=20, alpha=0.5)\nsns.histplot(imp.data['Engine'], kde=True, color='orange', label='Engine with MICE', bins=20, alpha=0.5)\nplt.title('Distribution of Engine: Original vs. MICE Imputed')\nplt.xlabel('Engine')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f11a3b44-5ef2-44eb-91ba-07103692a2e4",
   "metadata": {
    "name": "Sec02",
    "collapsed": false
   },
   "source": "**Section 2: Predicting Customer Spending**"
  },
  {
   "cell_type": "code",
   "id": "f061f187-6ead-4faf-9331-5246ad3c3508",
   "metadata": {
    "language": "python",
    "name": "LinReg"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ncustomers = pd.read_csv(\"customers.csv\")\n\n#get dummy variable columns\ncustomers = pd.get_dummies(data=customers, columns=['sex', 'race'], prefix=['sex','race'])\n\ncustomers[\"Asian_Female\"] = np.multiply(customers[\"race_asian\"],customers[\"sex_female\"])\ncustomers[\"Black_Female\"] = np.multiply(customers[\"race_black\"],customers[\"sex_female\"])\ncustomers[\"Hispanic_Female\"] = np.multiply(customers[\"race_hispanic\"],customers[\"sex_female\"])\ncustomers[\"Other_Female\"] = np.multiply(customers[\"race_other\"],customers[\"sex_female\"])\ncustomers[\"White_Female\"] = np.multiply(customers[\"race_white\"],customers[\"sex_female\"])\ncustomers[\"Asian_Male\"] = np.multiply(customers[\"race_asian\"],customers[\"sex_male\"])\ncustomers[\"Black_Male\"] = np.multiply(customers[\"race_black\"],customers[\"sex_male\"])\ncustomers[\"Hispanic_Male\"] = np.multiply(customers[\"race_hispanic\"],customers[\"sex_male\"])\ncustomers[\"Other_Male\"] = np.multiply(customers[\"race_other\"],customers[\"sex_male\"])\ncustomers[\"White_Male\"] = np.multiply(customers[\"race_white\"],customers[\"sex_male\"])\ncustomers[\"Asian_Other\"] = np.multiply(customers[\"race_asian\"],customers[\"sex_other\"])\ncustomers[\"Black_Other\"] = np.multiply(customers[\"race_black\"],customers[\"sex_other\"])\ncustomers[\"Hispanic_Other\"] = np.multiply(customers[\"race_hispanic\"],customers[\"sex_other\"])\ncustomers[\"Other_Other\"] = np.multiply(customers[\"race_other\"],customers[\"sex_other\"])\ncustomers[\"White_Other\"] = np.multiply(customers[\"race_white\"],customers[\"sex_other\"])\ncustomers.head()\n\n# create dependent and independent variables\nX = customers.drop(columns = ['spend', 'sex_female', 'sex_male', 'sex_other', 'race_asian', 'race_black', 'race_hispanic', 'race_other', 'race_white', 'Other_Other'])\ny = customers['spend']\n\n# Create the train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n\n# Linear regression\nlinreg = LinearRegression()\nmodel = linreg.fit(X_train, y_train)\n\n# Create a df of coefficients\nlinreg_coef = pd.DataFrame({\n    'Feature': X.columns,\n    'Coefficient': np.round(model.coef_, 3)\n})\n\nprint(linreg_coef)\n\n#calculate R2 on the test set\nlinreg_r2 = linreg.score(X_test, y_test)\nprint(\"R^2:\", linreg_r2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f44d40d5-a096-44e6-8697-fc73788a8ab5",
   "metadata": {
    "language": "python",
    "name": "Ridge"
   },
   "outputs": [],
   "source": "from sklearn.linear_model import Ridge, RidgeCV\n\n# scale X data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Create the train/test split\nX_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state=42)\n\n# Set range of alphas\nalphas = np.logspace(-3,3,50)\n\nridge_coef = []\n\nfor alpha in alphas:\n    ridge_model = Ridge(alpha=alpha)\n    ridge_model.fit(X_train_scaled, y_train)\n    ridge_coef.append(ridge_model.coef_)\n\nplt.figure(figsize = (10,6))\n\n# convert to numpy array\nridge_coef = np.array(ridge_coef)\n\n# Plot each feature's coefficient across alphas\nfor i in range(ridge_coef.shape[1]):\n    plt.plot(alphas, ridge_coef[:,i], label = X.columns[i])\n\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient Value')\nplt.title('Ridge Coefficients as a Function of Alpha')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Find optimal alpha\nridge_cv = RidgeCV(alphas=alphas, cv=None)  # Leave-One-Out CV by default\nridge_cv.fit(X_train_scaled, y_train)\nprint(\"Optimal Ridge alpha:\", ridge_cv.alpha_)\n\n# Fit final model with optimal alpha - 0.001\nridge_final = Ridge(alpha = 0.001)\nridge_final.fit(X_train_scaled, y_train)\n\nridge_coef = dict(zip(X.columns, ridge_final.coef_))\n\nfor feature, coef in ridge_coef.items():\n    print(f\"{feature}: {coef:.3f}\")\n\n#R^2 on test set\nridge_r2 = ridge_final.score(X_test_scaled, y_test)\nprint(\"Ridge R^2 on test set:\", ridge_r2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16c234f1-1107-4e27-a28e-d6c75e20be23",
   "metadata": {
    "language": "python",
    "name": "Lasso"
   },
   "outputs": [],
   "source": "# Set range of alphas\nalphas = np.logspace(-2,2,50)\n\nlasso_coef = []\n\nfor alpha in alphas:\n    lasso_model = Lasso(alpha=alpha, max_iter = 10000)\n    lasso_model.fit(X_train_scaled, y_train)\n    lasso_coef.append(lasso_model.coef_)\n\nplt.figure(figsize = (10,6))\n\n# convert to numpy array\nlasso_coef = np.array(lasso_coef)\n\n# Plot each feature's coefficient across alphas\nfor i in range(lasso_coef.shape[1]):\n    plt.plot(alphas, lasso_coef[:,i], label = X.columns[i])\n\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient Value')\nplt.title('Lasso Coefficients as a Function of Alpha')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nlasso_cv = LassoCV(alphas=alphas, cv = 5, max_iter=10000)\nlasso_cv.fit(X_train_scaled, y_train)\nprint(\"Optimal Lasso alpha:\", lasso_cv.alpha_)\n\n# Fit final model with optimal alpha - 0.001\nlasso_final = Lasso(alpha = 0.012)\nlasso_final.fit(X_train_scaled, y_train)\n\nlasso_coef = dict(zip(X.columns, lasso_final.coef_))\n\nfor feature, coef in lasso_coef.items():\n    print(f\"{feature}: {coef:.3f}\")\n\n#R^2 on test set\nlasso_r2 = lasso_final.score(X_test_scaled, y_test)\nprint(\"Lasso R^2 on test set:\", lasso_r2)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc3c656f-7dd8-47c3-8e68-4e4862d25dc3",
   "metadata": {
    "name": "Sec02_Response",
    "collapsed": false
   },
   "source": "Comparing the coefficients between the three models, the obvious similarity is that gender: female has a negative impact on spend for all races, gender: male has a positive impact. Most differences occur with gender: other. In the linear regression and ridge models, black and white other have a positive impact on spending, but hispanic and asian other have no impact. In the lasso model, white other has a slight negative impact on spending, and all other races of other are dropped from the model.\n\nThe linear regression and ridge models have almost the same R2, 86.33%. This makes sense considering the optimal alpha for the ridge model was 0.001, which means the model benefits from minimal regularization. The lasso model has the highest, but only slightly higher at 86.36%.\n\nI would choose the Lasso Model, as it has the highest R-Squared and the method removes any additional noise from the model and simplifies it by minimizing the impact of unnecessary coefficients.\n\nBoth the linear regression and the lasso model dropped Race: Asian and Hispanic of Gender: Other. The Lasso model also dropped Race: Black, Gender: Other. It did however include income, which was minimized in the linear regression model."
  }
 ]
}